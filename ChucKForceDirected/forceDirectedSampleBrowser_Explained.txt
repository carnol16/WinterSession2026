A interactive, audio-reactive 2D sound browser. It analyzes a folder of audio files, 
determines their sonic similarities, and organizes them visually so that similar
sounds group together in clusters.

Phase 1: Feature Extraction (The "Ears")
Goal: Convert raw audio into a "fingerprint" of numbers.

Mechanism: You use the FeatureCollector object to run four distinct analyses simultaneously on every sound file:

Centroid: Measures "Brightness" (Timbre).

Flux: Measures "Texture" or rate of change.

RMS: Measures "Loudness" or energy.

MFCCs (20 coeffs): Measures "Vowel Shape" or spectral envelope.

Result: Each sound file is converted into a single high-dimensional point (a vector with roughly 23 numbers).


Phase 2: K-Nearest Neighbors (The "Brain")
Goal: Find out which sounds are related to each other.

Mechanism: The script uses a KNN (K-Nearest Neighbors) algorithm. 
It looks at every sound's high-dimensional vector and finds the 3 closest matches in the dataset.

Result: It builds a "Graph" (a web of connections). 
If Sound A is similar to Sound B, a connection is recorded. This captures the topology (shape) of your data.


Phase 3: Physics Simulation (The "Eyes")
Goal: Visualize these high-dimensional relationships on a 2D screen.

Mechanism: The script uses a Force-Directed Physics Engine:

Initialization: All points are thrown onto the screen at random positions.

Attraction: Points that are "neighbors" (found in Phase 2) pull each other closer.

Repulsion: All points push each other away to prevent overlap.

Gravity: A gentle force pulls everyone toward the center so they don't drift off-screen.

Damping: Friction is applied to stop the points from moving too fast.

Result: Over time, the pushing and pulling balances out, and clusters emerge.
Sounds that are similar naturally clump together on screen, creating a map you can explore with your mouse.


